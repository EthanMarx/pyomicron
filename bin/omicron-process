#!/usr/bin/env python
# Copyright (C) Duncan Macleod (2016)
#
# This file is part of LIGO-Omicron.
#
# LIGO-Omicron is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# LIGO-Omicron is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with LIGO-Omicron.  If not, see <http://www.gnu.org/licenses/>.

"""Process LIGO data using the Omicron event trigger generator (ETG)

This utility can be used to process one or more channels or LIGO data using
Omicron with minimal manual labour in determining segments, finding data,
and configuring HTCondor.

The input to this should be an INI-format configuration file that lists the
processing parameters and channels that pass to Omicron, something like:

```ini
[GW]
q-range = 3.3166 150
frequency-range = 4.0 8192.0
frametype = H1_HOFT_C00
state-flag = H1:DMT-CALIBRATED:1
sample-frequency = 16384
chunk-duration = 124
segment-duration = 64
overlap-duration = 4
mismatch-max = 0.2
snr-threshold = 5
channels = H1:GDS-CALIB_STRAIN
```

The above 'GW' group name should then be passed to `omicron-process` along
with any customisations available from the command line, e.g.

```
omicron-process GW --config-file ./config.ini
```

By default `omicron-process` will look at the most recent data available
('online' mode), to run in 'offline' mode, pass the `--gps` argument

```
omicron-process GW --config-file ./config.ini --gps <gpsstart> <gpsstop>
```

The output of `omicron-process` is a Directed Acyclic Graph (DAG) that is
*automatically* submitted to condor for processing.

"""

from __future__ import print_function

import argparse
import os
import sys
import shutil
import re
from time import sleep
from glob import glob
from getpass import getuser

# python 3
try:
    import configparser
# python 2
except ImportError:
    import ConfigParser as configparser

from glue import pipeline
from glue.lal import CacheEntry

from omicron import (const, segments, log, data, parameters, utils, condor)

logger = log.Logger('omicron-process')

# -- parse command line
epilog = """This source code for this project is available here:

https://git.ligo.org/detchar/ligo-omicron/

All issues regarding this software should be raised using GitLab, bug reports and feature requests are encouraged.
"""

parser = argparse.ArgumentParser(
    description=__doc__,
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog=epilog)
parser._positionals.title = 'Positional arguments'
parser._optionals.title = 'Optional arguments'

parser.add_argument('group', help='name of configuration group to process')
parser.add_argument('-t', '--gps', nargs=2, type=int,
                    metavar='GPSTIME',
                    help='GPS times for offline processing')
parser.add_argument('-f', '--config-file', default=const.OMICRON_CHANNELS_FILE,
                    help='path to configuration file, default: %(default)s')
parser.add_argument('-i', '--ifo', default=const.IFO,
                    help='IFO prefix to process, default: %(default)s')

outg = parser.add_argument_group('Output options')
outg.add_argument('-o', '--output-dir',
                  help='path to output directory, '
                       'default under ~/Omicron/Prod somewhere')
outg.add_argument('-a', '--archive', action='store_true', default=False,
                  help='archive created files under %s, default: %%(default)s'
                       % const.OMICRON_ARCHIVE)
outg.add_argument('-g', '--file-tag', default='',
                  help='additional file tag to be appended to final '
                       'file descriptions')

procg = parser.add_argument_group('Processing options')
procg.add_argument('-C', '--max-chunks-per-job', type=int, default=4,
                   help='maximum number of chunks to process in a single '
                        'condor job, default: %(default)s')
procg.add_argument('-N', '--max-channels-per-job', type=int, default=10,
                   help='maximum number of channels to process in a single '
                        'condor job, default: %(default)s')

condorg = parser.add_argument_group('Condor options')
condorg.add_argument('--no-submit', action='store_true', default=False,
                     help='do not submit the DAG to condor, '
                          'default: %(default)s')
condorg.add_argument('--universe', default='vanilla',
                     choices=['vanilla', 'local'],
                     help='condor universe, default: %(default)s')
condorg.add_argument('--executable', default=utils.which('omicron.exe'),
                     help='omicron executable, default: %(default)s')
condorg.add_argument('--condor-retry', type=int, default=2,
                     help='number of times to retry each job if failed, '
                          'default: %(default)s')
condorg.add_argument('--condor-accounting-group',
                     default='ligo.dev.o1.detchar.transient.omicron',
                     help='accounting_group for condor submission on the LIGO '
                          'Data Grid, default: %(default)s')
condorg.add_argument('--condor-accounting-group-user', default=getuser(),
                     help='accounting_group_user for condor submission on the '
                          'LIGO Data Grid, default: %(default)s')
condorg.add_argument('--submit-rescue-dag', type=int, default=0,
                     help='number of times to automatically submit the '
                          'rescue DAG, default: %(default)s')
condorg.add_argument('-c', '--condor-command', action='append', type=str,
                     default=[],
                     help="Extra condor submit commands to add to "
                          "gw_summary submit file. Can be given "
                          "multiple times in the form \"key=value\"")

pipeg = parser.add_argument_group('Pipeline options')
pipeg.add_argument('--skip-omicron', action='store_true', default=False,
                   help='skip running omicron, default: %(default)s')
pipeg.add_argument('--skip-root-merge', action='store_true', default=False,
                   help='skip running omicron-root-merge, '
                        'default: %(default)s')
pipeg.add_argument('--skip-ligolw_add', action='store_true', default=False,
                   help='skip running ligolw_add, default: %(default)s')
pipeg.add_argument('--skip-gzip', action='store_true', default=False,
                   help='skip running gzip, default: %(default)s')
pipeg.add_argument('--skip-postprocessing', action='store_true', default=False,
                   help='skip all post-processing, equivalent to '
                        '--skip-root-merge --skip-lioglw_add --skip-gzip, '
                        'default: %(default)s')

args = parser.parse_args()

# validate command line arguments
if args.ifo is None:
    parser.error("Cannot determine IFO prefix from sytem, "
                 "please pass --ifo on the command line")
if args.executable is None:
    parser.error("Cannot find omicron.exe on path, please pass "
                 "--executable on the command line")

if all((args.skip_root_merge, args.skip_ligolw_add, args.skip_gzip,
        not args.archive)):
    args.skip_postprocessing = True

ifo = args.ifo
llhoft = data.ligo_low_latency_hoft_type(ifo)
group = args.group
online = args.gps is None
filetag = args.file_tag
if filetag and not filetag.startswith('_'):
    filetag = '_%s' % filetag
filetag = re.sub('[:_\s-]', '_', filetag.upper())

logger.info("--- Welcome to the Omicron processor ---")

tempfiles = []

# -- parse configuration file and get parameters ------------------------------

cp = configparser.ConfigParser()
cp.read(args.config_file)

# validate
if not cp.has_section(group):
    raise configparser.NoSectionError(group)

# get params
frametype = cp.get(group, 'frametype')
logger.debug("frametype = %s" % frametype)
chunkdur = cp.getint(group, 'chunk-duration')
logger.debug("chunkdur = %s" % chunkdur)
segdur = cp.getint(group, 'segment-duration')
logger.debug("segdur = %s" % segdur)
overlap = cp.getint(group, 'overlap-duration')
logger.debug("overlap = %s" % overlap)
padding = int(overlap / 2)
logger.debug("padding = %s" % padding)
frange = map(float, cp.get(group, 'frequency-range').split())
logger.debug('frequencyrange = [%s, %s)' % tuple(frange))
sampling = cp.getfloat(group, 'sample-frequency')
logger.debug('samplingfrequency = %s' % sampling)
parameters.validate_parameters(chunkdur, segdur, overlap, frange, sampling)
try:
    stateflag = cp.get(group, 'state-flag')
except configparser.NoOptionError:
    statechannel = None
else:
    logger.debug("State flag = %s" % stateflag)
    try:
        statechannel = segments.STATE_CHANNEL[stateflag]
    except KeyError as e:
        e.args = ('Cannot map state flag %r to channel' % stateflag,)
        raise
    logger.debug("State channel = %s" % statechannel)

rundir = utils.get_output_directory(args)

# -- find run segment

segfile = os.path.join(rundir, 'segments.txt')
if online:
    if frametype == '%s_HOFT_C00' % ifo:
        end = data.get_latest_data_gps(ifo, llhoft)
    else:
        end = data.get_latest_data_gps(ifo, frametype)
    end -= padding
    try:
        start = segments.get_last_run_segment(segfile)[1]
    except IOError:
        logger.debug("No online segment record, starting with 4000 seconds")
        start = end - 4000
    else:
        logger.debug("Online segment record recovered")
else:
    start, end = args.gps
rundir = os.path.abspath(rundir)
segfile = os.path.join(rundir, 'segments.txt')

duration = end - start
datastart = start - padding
dataend = end + padding
dataduration = dataend - datastart

logger.info("Processing segment determined as")
logger.info("    %d %d" % (datastart, dataend))
logger.info("Duration = %d seconds" % dataduration)

span = (start, end)

# -- double-check frametype for h(t)
# don't use aggregated h(t) if running online

if frametype == '%s_HOFT_C00' % ifo:
    try:
        data.check_data_availability(ifo, frametype, start, end)
    except RuntimeError:
        msg = ("Gaps found in %s availability, turning to %s"
               % (frametype, llhoft))
        if online:
            logger.debug(msg)
        else:
            logger.warning(msg)
        frametype = llhoft

# -- set directories and check for an existing process ------------------------

if not os.path.isdir(rundir):
    os.makedirs(rundir)
logger.info("Using run directory\n%s" % rundir)

if os.path.isfile(os.path.join(rundir, 'omicron.dag.lock')):
    raise RuntimeError("omicron.dag.lock found in %s, DAG already running"
                       % rundir)
elif any(map(os.path.isfile, glob(
        os.path.join(rundir, 'omicron.dag.rescue[0-9][0-9][0-9]')))):
    raise RuntimeError("rescue DAG found in %s, will not continue" % rundir)

# -- find segments and frame files --------------------------------------------

# validate span is long enough
if dataduration < chunkdur:
    logger.info("Segment is too short (%d < %d), please try again later"
                % (duration, chunkdur - padding * 2))
    sys.exit(0)

# find run segments
if statechannel:
    logger.info("Finding segments for relevant state...")
    segs = segments.get_state_segments(statechannel, frametype,
                                       datastart, dataend)
else:
    segs = segments.get_frame_segments(ifo, frametype, datastart, dataend)

if len(segs):
    logger.info("State/frame segments recovered as")
    for seg in segs:
        logger.info("    %d %d" % seg)
    logger.info("Duration = %d seconds" % abs(segs))

# truncate final segment if running online
try:
    lastseg = segs[-1]
except IndexError:
    truncate = False
else:
    truncate = online and lastseg[1] == dataend

# if segment is shorter than one chunk, leave it until later
if truncate and (
        (not statechannel and abs(lastseg) < chunkdur) or
        (statechannel and abs(lastseg) < (chunkdur + segdur))):
    logger.info("The final segment is too short, but ends at the limit of "
                "available data, presumably this is an active segment. "
                "It will be removed so that it can be "
                "processed properly later")
    segs = segs[:-1]
    dataend = lastseg[0]
# if long enough and no state required, restrict to an integer number of chunks
elif truncate and (not statechannel or abs(lastseg) >= (chunkdur + segdur)):
    logger.info("Truncating to process only complete chunks...")
    t, e = lastseg
    step = chunkdur - overlap
    while t + chunkdur <= e:
        t += step
    segs[-1] = type(segs[-1])(lastseg[0], t + overlap)
    dataend = segs[-1][1]
    logger.info("This analysis will now run to %d" % dataend)

dataspan = type(segs)([segments.Segment(datastart, dataend)])

# find the frames
cache = data.find_frames(ifo, frametype, datastart-overlap, dataend+overlap,
                         on_gaps='warn')
if not online and len(cache) == 0:
    raise RuntimeError("No frames found for %s-%s" % (ifo[0], frametype))
try:
    cachesegs = (segments.cache_segments(cache) & dataspan).coalesce()
except TypeError:
    alldata = False
else:
    alldata = cachesegs == dataspan

# if all of the data are available, but no segments, record segments.txt
if len(segs) == 0 and online and alldata:
    logger.info("No segments found, but all data are available. "
                "A segments.txt file will be written so we don't have to "
                "search these data again")
    segments.write_segments(cachesegs, segfile)
    logger.info("Segments written to\n%s" % segfile)
    sys.exit(0)
# otherwise not all data are available, so
elif len(segs) == 0 and online:
    logger.info("No segments found, please try again later")
    sys.exit(0)
elif len(segs) == 0:
    raise RuntimeError("No segments found")

# apply minimum duration requirement
segs = type(segs)(s for s in segs if abs(s) >= segdur)
# and calculate trigger output segments
trigsegs = type(segs)(type(s)(*s) for s in segs).contract(padding)

# display segments
logger.info("Final data segments selected as")
for seg in segs:
    logger.info("    %d %d" % seg)
logger.info("Duration = %d seconds" % abs(segs))

span = type(trigsegs)([trigsegs.extent()])

logger.info("This will output triggers for")
for seg in trigsegs:
    logger.info("    %d %d" % seg)
logger.info("Duration = %d seconds" % abs(trigsegs))


# write cache
cachedir = os.path.join(rundir, 'cache')
if not os.path.isdir(cachedir):
    os.makedirs(cachedir)
cachefile = os.path.join(cachedir, 'frames.lcf')
data.write_cache(cache, cachefile)

# -- make parameters files then generate the DAG ------------------------------

condir = os.path.join(rundir, 'condor')
if not os.path.isdir(condir):
    os.makedirs(condir)

# generate a 'master' parameters.txt file for archival purposes
tmpparfile = parameters.generate_parameters_files(
    cp, group, cachefile, rundir, channellimit=int(1e8))[0][0]
parfile = os.path.join(rundir, 'parameters.txt')
logger.debug("Created master parameters file\n%s" % parfile)
shutil.move(tmpparfile, parfile)

# then generate actual parameters files for submission (channel groups)
jobfiles = parameters.generate_parameters_files(
    cp, group, cachefile, rundir, channellimit=args.max_channels_per_job)

# create log directory
logdir = os.path.join(rundir, 'logs')
if not os.path.isdir(logdir):
    os.mkdir(logdir)

# create dag
dag = pipeline.CondorDAG(os.path.join(logdir, 'omicron.log'))
dag.set_dag_file(os.path.join(condir, 'omicron'))

# set up condor commands for all jobs
condorcmds = {'accounting_group': args.condor_accounting_group,
              'accounting_group_user': args.condor_accounting_group_user}
for cmd_ in args.condor_command:
    key, value = cmd_.split('=', 1)
    condorcmds[key.rstrip().lower()] = value.strip()

# create omicron job
ojob = condor.OmicronProcessJob(args.universe, args.executable, subdir=condir,
                                logdir=logdir, **condorcmds)
ojob.add_condor_cmd('+OmicronProcess', '"%s"' % group)
nperjob = args.max_chunks_per_job

# create post-processing job
ppjob = condor.OmicronProcessJob(args.universe, utils.which('bash'),
                                 subdir=condir, logdir=logdir,
                                 tag='post-processing', **condorcmds)
ppjob.add_condor_cmd('+OmicronPostProcess', '"%s"' % group)
ppjob.add_short_opt('e', '')
rootmerge = utils.which('omicron-root-merge')
ligolw_add = utils.which('ligolw_add')
gzip = utils.which('gzip')

for s, e in segs:
    # work out job segments
    filesegments = segments.omicron_output_segments(
        s, e, chunkdur, segdur, overlap)

    # separate long segments into shorter chunks
    nodesegs = segments.parallel_omicron_segments(
        s, e, chunkdur, overlap, nperjob)

    # build node for each parameter file
    for i, (pf, chanlist) in enumerate(jobfiles):
        nodes = []
        for subseg in nodesegs:
            # process
            if not args.skip_omicron:
                node = pipeline.CondorDAGNode(ojob)
                node.set_category('omicron')
                node.set_retry(str(args.condor_retry))
                node.add_var_arg(str(subseg[0]))
                node.add_var_arg(str(subseg[1]))
                node.add_var_arg(os.path.abspath(pf))
                dag.add_node(node)
                nodes.append(node)

        # post-process
        if not args.skip_postprocessing:
            script = os.path.join(
                condir, 'post-process-%d-%d-%d.sh' % (s, e, i))
            with open(script, 'w') as f:
                print('#!/bin/bash -e\n', file=f)
                # build post-processing nodes for each channel
                for c in chanlist:
                    print("# %s" % c, file=f)
                    chandir = os.path.join(rundir, 'triggers', c)
                    cname = re.sub('[:_-]', '_', c).replace('_', '-', 1)
                    roots = ' '.join(
                        os.path.join(chandir, '%s_%d_%d.root' % (c, fs, fe-fs))
                        for (fs, fe) in filesegments)
                    # add omicron-root-merge
                    if args.skip_root_merge:
                        root = roots
                    else:
                        root = os.path.join(
                            chandir, '%s%s_OMICRON-%d-%d.root'
                                     % (cname, filetag, s, e - s))
                        print('%s %s %s --strict'
                              % (rootmerge, roots, root), file=f)
                    # add ligolw_add
                    xmls = ' '.join(os.path.join(
                        chandir, '%s_Omicron-%d-%d.xml' % (cname, fs, fe-fs))
                        for (fs, fe) in filesegments)
                    if args.skip_ligolw_add:
                        xml = xmls
                    else:
                        xml = os.path.join(
                            chandir, '%s%s_OMICRON-%d-%d.xml'
                                     % (cname, filetag, s, e - s))
                        print('%s %s --output %s'
                              % (ligolw_add, xmls, xml), file=f)
                    # add gzip
                    if not args.skip_gzip:
                        print('%s %s' % (gzip, xml), file=f)
                        xml = '%s.gz' % xml

                    # add archive
                    if args.archive:
                        x = CacheEntry.from_T050017(xml)
                        target = os.path.join(const.OMICRON_ARCHIVE,
                                              x.observatory,
                                              x.description,
                                              str(int(x.segment[0]))[:5])
                        print('mkdir -p %s' % target, file=f)
                        print('mv %s %s %s\n' % (xml, root, target), file=f)

                    # add rm jobs
                    print('rm -f %s %s' % (roots, xmls), file=f)
            ppnode = pipeline.CondorDAGNode(ppjob)
            ppnode.add_var_arg(script)
            ppnode.set_category('postprocessing')
            if not args.skip_omicron:
                for node in nodes:
                    ppnode.add_parent(node)
            dag.add_node(ppnode)
            tempfiles.append(script)

dag.write_sub_files()
dag.write_dag()
dag.write_script()
dagfile = dag.get_dag_file()
with open(dagfile, 'a') as f:
    print('DOT %s.dot' % os.path.splitext(dagfile)[0], file=f)
logger.info("Dag with %d nodes written to" % len(dag.get_nodes()))
print(os.path.abspath(dagfile))

if args.no_submit:
    sys.exit(0)

# -- submit the DAG and babysit -----------------------------------------------

# submit DAG
logger.info("--- Submitting DAG to condor -------")

for i in range(args.submit_rescue_dag + 1):
    dagid = condor.submit_dag(dagfile, force=i == 0)
    logger.info("Condor ID = %d" % dagid)
    logger.debug("Sleep for 15 seconds while DAGMan starts up")
    sleep(15)
    # find lock file and monitor
    logger.info("Monitoring DAG via ID...")
    states = ['unready', 'ready', 'queued', 'failed', 'done']
    colors = ['white', 'magenta', 'blue', 'red', 'green']
    old = dict((k, -1) for k in states)
    logger.debug("Dag Status (%d nodes total):" % (len(dag.get_nodes())))
    logger.debug('-' * 47)
    logger.debug(' {0} |  {1} |  {2} |  {3} |    {4}'.format(
             *[log.color_text(s, c) for s, c in zip(states, colors)]))
    logger.debug('-' * 47)
    for job in condor.iterate_dag_status(dagid):
        for state in states:
            if job[state] != old[state]:
                logger.debug(' | '.join(['%7s' % job[s] for s in states]))
                break
        old = job
    logger.debug('-' * 47)
    if job['exitcode']:
        logger.critical("DAG has exited with status %d" % job['exitcode'])
    else:
        logger.info("DAG has exited with status %d" % job['exitcode'])
    if job['exitcode'] == 0:
        break
    elif i == args.submit_rescue_dag:
        raise RuntimeError("DAG has failed to complete %d times"
                           % (args.submit_rescue_dag + 1))
    else:
        sleep(2)
        rescue = condor.find_rescue_dag(dagfile)
        logger.warning("Rescue DAG %s was generated" % rescue)

# write segments
segments.write_segments(span, segfile)
logger.info("Segments written to\n%s" % segfile)

# archive files
stub = '%d-%d' % (start, end)
for f in ['%s.dagman.out' % dagfile, segfile, cachefile, parfile]:
    base, ext = os.path.splitext(os.path.basename(f))
    archive = os.path.join(logdir, '%s-%s%s' % (base, stub, ext))
    shutil.copyfile(f, archive)
    logger.debug("Archived file\n%s --> %s" % (f, archive))

# clean up temporary files
tempfiles.extend(glob(os.path.join(rundir, 'triggers', 'ffconvert.*.ffl')))
for f in tempfiles:
    os.remove(f)
    logger.debug('Deleted file %r' % f)

# and exit
logger.info("---- Processing complete -----------")
