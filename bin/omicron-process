#!/usr/bin/env python
# Copyright (C) Duncan Macleod (2016)
#
# This file is part of LIGO-Omicron.
#
# LIGO-Omicron is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# LIGO-Omicron is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with LIGO-Omicron.  If not, see <http://www.gnu.org/licenses/>.

"""Process Omicron triggers online for LIGO
"""

from __future__ import print_function

import argparse
import os
import sys
import shutil
import re
from time import sleep
from glob import glob
from getpass import getuser

# python 3
try:
    import configparser
# python 2
except ImportError:
    import ConfigParser as configparser

from glue import pipeline
from glue.lal import CacheEntry

from omicron import (const, segments, log, data, parameters, utils, condor)

logger = log.Logger('omicron-process')

# -- parse command line
parser = argparse.ArgumentParser(description=__doc__)

parser.add_argument('group', help='name of configuration group to process')
parser.add_argument('-f', '--config-file', default=const.OMICRON_CHANNELS_FILE,
                    help='path to configuration file, default: %(default)s')
parser.add_argument('-i', '--ifo', default=const.IFO,
                    help='IFO prefix to process, default: %(default)s')
parser.add_argument('-t', '--gps', nargs=2, type=int,
                    help='GPS times for offline processing')

outg = parser.add_argument_group('Output options')
outg.add_argument('-o', '--output-dir',
                  help='path to output directory, '
                       'default under ~/Omicron/Prod somewhere')
outg.add_argument('-a', '--archive', action='store_true', default=False,
                  help='archive created files under %s, default: %%(default)s'
                       % const.OMICRON_ARCHIVE)

condorg = parser.add_argument_group('Condor options')
condorg.add_argument('--no-submit', action='store_true', default=False,
                     help='do not submit the DAG to condor, '
                          'default: %(default)s')
condorg.add_argument('--universe', default='vanilla',
                     help='condor universe, default: %(default)s')
condorg.add_argument('--executable', default=utils.which('omicron.exe'),
                     help='omicron executable, default: %(default)s')
condorg.add_argument('--condor-retry', type=int, default=2,
                     help='number of times to retry each job if failed, '
                          'default: %(default)s')
condorg.add_argument('--condor-accounting-group',
                     default='ligo.dev.o1.detchar.transient.omicron',
                     help='accounting_group for condor submission on the LIGO '
                          'Data Grid, default: %(default)s')
condorg.add_argument('--condor-accounting-group-user', default=getuser(),
                     help='accounting_group_user for condor submission on the '
                          'LIGO Data Grid, default: %(default)s')
condorg.add_argument('--submit-rescue-dag', type=int, default=0,
                     help='number of times to automatically submit the '
                          'rescue DAG, default: %(default)s')

args = parser.parse_args()

if args.ifo is None:
    raise parser.error("Cannot determine IFO prefix from sytem"
                       ", please pass --ifo on the command line")
ifo = args.ifo
llhoft = data.ligo_low_latency_hoft_type(ifo)
group = args.group
online = args.gps is None

logger.info("--- Welcome to the Omicron processor ---")

tempfiles = []

# -- parse configuration file and get parameters ------------------------------

cp = configparser.ConfigParser()
cp.read(args.config_file)

# validate
if not cp.has_section(group):
    raise configparser.NoSectionError(group)

# get params
frametype = cp.get(group, 'frametype')
logger.debug("frametype = %s" % frametype)
chunkdur = cp.getint(group, 'chunk-duration')
logger.debug("chunkdur = %s" % chunkdur)
segdur = cp.getint(group, 'segment-duration')
logger.debug("segdur = %s" % segdur)
overlap = cp.getint(group, 'overlap-duration')
logger.debug("overlap = %s" % overlap)
if overlap % 2:
    raise ValueError("Cannot proceed with odd overlap (%d seconds)" % overlap)
padding = int(overlap / 2)
logger.debug("padding = %s" % padding)
try:
    stateflag = cp.get(group, 'state-flag')
except configparser.NoOptionError:
    statechannel = None
else:
    logger.debug("State flag = %s" % stateflag)
    try:
        statechannel = segments.STATE_CHANNEL[stateflag]
    except KeyError as e:
        e.args = ('Cannot map state flag %r to channel' % stateflag,)
        raise
    logger.debug("State channel = %s" % statechannel)

rundir = utils.get_output_directory(args)

# -- find run segment

segfile = os.path.join(rundir, 'segments.txt')
if online:
    if frametype == '%s_HOFT_C00' % ifo:
        end = data.get_latest_data_gps(ifo, llhoft)
    else:
        end = data.get_latest_data_gps(ifo, frametype)
    end -= padding
    try:
        start = segments.get_last_run_segment(segfile)[1]
    except IOError:
        logger.debug("No online segment record, starting with 4000 seconds")
        start = end - 4000
    else:
        logger.debug("Online segment record recovered")
else:
    start, end = args.gps
rundir = os.path.abspath(rundir)
segfile = os.path.join(rundir, 'segments.txt')

duration = end - start
datastart = start - padding
dataend = end + padding
dataduration = dataend - datastart

logger.info("Processing segment determined as")
logger.info("    %d %d" % (datastart, dataend))
logger.info("Duration = %d seconds" % dataduration)

span = (start, end)

# -- double-check frametype for h(t)
# don't use aggregated h(t) if running online

if frametype == '%s_HOFT_C00' % ifo:
    try:
        data.check_data_availability(ifo, frametype, start, end)
    except RuntimeError:
        msg = ("Gaps found in %s availability, turning to %s"
               % (frametype, llhoft))
        if online:
            logger.debug(msg)
        else:
            logger.warning(msg)
        frametype = llhoft

# -- set directories and check for an existing process ------------------------

if not os.path.isdir(rundir):
    os.makedirs(rundir)
logger.info("Using run directory\n%s" % rundir)

if os.path.isfile(os.path.join(rundir, 'omicron.dag.lock')):
    raise RuntimeError("omicron.dag.lock found in %s, DAG already running"
                       % rundir)
elif any(map(os.path.isfile, glob(
        os.path.join(rundir, 'omicron.dag.rescue[0-9][0-9][0-9]')))):
    raise RuntimeError("rescue DAG found in %s, will not continue" % rundir)

# -- find segments and frame files --------------------------------------------

# validate span is long enough
if dataduration < chunkdur:
    logger.info("Segment is too short (%d < %d), please try again later"
             % (duration, chunkdur - padding * 2))
    sys.exit(0)

# find run segments
if statechannel:
    logger.info("Finding segments for relevant state...")
    segs = segments.get_state_segments(statechannel, frametype,
                                       datastart, dataend)
else:
    segs = segments.get_frame_segments(ifo, frametype, datastart, dataend)

if len(segs):
    logger.info("State/frame segments recovered as")
    for seg in segs:
        logger.info("    %d %d" % seg)
    logger.info("Duration = %d seconds" % abs(segs))

# truncate final segment if running online
try:
    lastseg = segs[-1]
except IndexError:
    truncate = False
else:
    truncate = online and lastseg[1] == dataend

# if segment is shorter than one chunk, leave it until later
if truncate and (
        (not statechannel and abs(lastseg) < chunkdur) or
        (statechannel and abs(lastseg) < (chunkdur + segdur))):
    logger.info("The final segment is too short, but ends at the limit of "
                "available data, presumably this is an active segment. "
                "It will be removed so that it can be "
                "processed properly later")
    segs = segs[:-1]
    dataend = lastseg[0]
# if long enough and no state required, restrict to an integer number of chunks
elif truncate and (not statechannel or abs(lastseg) >= (chunkdur + segdur)):
    logger.info("Truncating to process only complete chunks...")
    t, e = lastseg
    step = chunkdur - overlap
    while t + chunkdur <= e:
        t += step
    segs[-1] = type(segs[-1])(lastseg[0], t + overlap)
    dataend = segs[-1][1]
    logger.info("This analysis will now run to %d" % dataend)

dataspan = type(segs)([segments.Segment(datastart, dataend)])

# find the frames
cache = data.find_frames(ifo, frametype, datastart, dataend, on_gaps='warn')
if not online and len(cache) == 0:
    raise RuntimeError("No frames found for %s-%s" % (ifo[0], frametype))
try:
    cachesegs = (segments.cache_segments(cache) & dataspan).coalesce()
except TypeError:
    alldata = False
else:
    alldata = cachesegs == dataspan

# if all of the data are available, but no segments, record segments.txt
if len(segs) == 0 and online and alldata:
    logger.info("No segments found, but all data are available. "
                "A segments.txt file will be written so we don't have to "
                "search these data again")
    segments.write_segments(cachesegs, segfile)
    logger.info("Segments written to\n%s" % segfile)
    sys.exit(0)
# otherwise not all data are available, so 
elif len(segs) == 0 and online:
    logger.info("No segments found, please try again later")
    sys.exit(0)
elif len(segs) == 0:
    raise RuntimeError("No segments found")

# apply minimum duration requirement
segs = type(segs)(s for s in segs if abs(s) >= segdur)
# and calculate trigger output segments
trigsegs = type(segs)(type(s)(*s) for s in segs).contract(padding)

# display segments
logger.info("Final data segments selected as")
for seg in segs:
    logger.info("    %d %d" % seg)
logger.info("Duration = %d seconds" % abs(segs))

span = type(trigsegs)([trigsegs.extent()])

logger.info("This will output triggers for")
for seg in trigsegs:
    logger.info("    %d %d" % seg)
logger.info("Duration = %d seconds" % abs(trigsegs))


# write cache
cachefile = os.path.join(rundir, 'frames.lcf')
data.write_cache(cache, cachefile)

# -- make parameters files then generate the DAG ------------------------------

condir = os.path.join(rundir, 'condor')
if not os.path.isdir(condir):
    os.makedirs(condir)

# generate a 'master' parameters.txt file for archival purposes
tmpparfile = parameters.generate_parameters_files(
    cp, group, cachefile, rundir, channellimit=int(1e8))[0][0]
parfile = os.path.join(rundir, 'parameters.txt')
logger.debug("Create master parameters file\n%s" % parfile)
shutil.move(tmpparfile, parfile)

# then generate actual parameters files for submission (channel groups)
jobfiles = parameters.generate_parameters_files(cp, group, cachefile, rundir)

logdir = os.path.join(rundir, 'logs')
if not os.path.isdir(logdir):
    os.mkdir(logdir)

dag = pipeline.CondorDAG(os.path.join(logdir, 'omicron.log'))
dag.set_dag_file(os.path.join(condir, 'omicron'))

accounting = {'accounting_group': args.condor_accounting_group,
              'accounting_group_user': args.condor_accounting_group_user}

# get omicron job
ojob = condor.OmicronProcessJob(args.universe, args.executable, subdir=condir,
                                logdir=logdir, **accounting)
ojob.add_condor_cmd('+OmicronGroup', '"%s"' % group)

# get merge job
mjob = condor.OmicronProcessJob(args.universe,
                                utils.which('omicron-root-merge'),
                                subdir=condir, logdir=logdir, **accounting)
mjob.add_opt('strict', '')
mjob.add_opt('remove-input', '')

# get ligolw_add job
xjob = condor.OmicronProcessJob(args.universe, utils.which('ligolw_add'),
                                subdir=condir, logdir=logdir, **accounting)
xjob.add_opt('remove-input', '')

# get gzip job
zjob = condor.OmicronProcessJob(args.universe, utils.which('gzip'),
                                subdir=condir, logdir=logdir, **accounting)

# get archive job
mv = utils.which('mv')
vjob = condor.OmicronProcessJob(args.universe, utils.which('bash'),
                                subdir=condir, logdir=logdir, **accounting)

for s, e in segs:
    # work out job segments
    filesegments = segments.SegmentList()
    t = s + padding
    while t < e - padding:
        e_ = min(t + chunkdur-overlap, e - padding)
        filesegments.append(segments.Segment(t, e_))
        t = e_

    gzipnodes = []
    xmlfiles = []
    rootfiles = {}

    # build node for each parameter file
    for pf, chanlist in jobfiles:
        node = pipeline.CondorDAGNode(ojob)
        node.set_category('omicron')
        node.set_retry(str(args.condor_retry))
        node.add_var_arg(str(s))
        node.add_var_arg(str(e))
        node.add_var_arg(os.path.abspath(pf))
        dag.add_node(node)
        # build post-processing nodes for each channel
        for c in chanlist:
            chandir = os.path.join(rundir, 'triggers', c)
            cname = re.sub('[:_-]', '_', c).replace('_', '-', 1)
            # add omicron-root-merge node
            out = os.path.join(chandir, '%s_OMICRON-%d-%d.root'
                                        % (cname, span[0][0], abs(span)))
            mnode = pipeline.CondorDAGNode(mjob)
            mnode.set_category('omicron-root-merge')
            mnode.add_parent(node)
            mnode.add_var_arg(' '.join(
                os.path.join(chandir, '%s_%d_%d.root' % (c, fs, fe-fs)) for
                (fs, fe) in filesegments))
            mnode.add_var_arg(out)
            rootfiles[out] = mnode
            dag.add_node(mnode)
            # add ligolw_add node
            out = os.path.join(chandir, '%s_OMICRON-%d-%d.xml'
                                        % (cname, span[0][0], abs(span)))
            xnode = pipeline.CondorDAGNode(xjob)
            xnode.set_category('ligolw_add')
            xnode.add_parent(node)
            xnode.add_var_arg(' '.join(
                os.path.join(chandir,
                             '%s_Omicron-%d-%d.xml' % (cname, fs, fe-fs)) for
                (fs, fe) in filesegments))
            xnode.add_var_opt('output', out)
            dag.add_node(xnode)
            xmlfiles.append(out)
            gzipnodes.append(xnode)

    # gzip node
    znode = pipeline.CondorDAGNode(zjob)
    znode.set_category('gzip')
    for node in gzipnodes:
        znode.add_parent(node)
    znode.add_var_arg(' '.join(xmlfiles))
    dag.add_node(znode)

    # build archive nodes
    mvscript = os.path.join(condir, 'mv-%d-%d.sh' % (s, e-s))
    vnode = pipeline.CondorDAGNode(vjob)
    vnode.add_var_arg(mvscript)
    with open(mvscript, 'w') as f:
        print('#!/bin/bash -e', file=f)
        gzipfiles = ['%s.gz' % xml for xml in xmlfiles]
        for xml, root in zip(gzipfiles, rootfiles):
            e = CacheEntry.from_T050017(xml)
            target = os.path.join(const.OMICRON_ARCHIVE, e.observatory,
                                  e.description, str(int(e.segment[0]))[:5])
            print('mkdir -p %s' % target, file=f)
            print('%s %s %s %s\n' % (mv, xml, root, target), file=f)
            vnode.add_parent(rootfiles[root])
    vnode.add_parent(znode)
    vnode.set_category('archive')
    dag.add_node(vnode)
    tempfiles.append(mvscript)

dag.write_sub_files()
dag.write_dag()
dag.write_script()
dagfile = dag.get_dag_file()
with open(dagfile, 'a') as f:
    print('DOT %s.dot' % os.path.splitext(dagfile), file=f)
logger.info("Dag with %d nodes written to" % len(dag.get_nodes()))
print(os.path.abspath(dagfile))

if args.no_submit:
    sys.exit(0)

# -- submit the DAG and babysit -----------------------------------------------

# submit DAG
logger.info("--- Submitting DAG to condor -------")

for i in range(args.submit_rescue_dag + 1):
    dagid = condor.submit_dag(dagfile, force=i==0)
    logger.info("Condor ID = %d" % dagid)
    logger.debug("Sleep for 15 seconds while DAGMan starts up")
    sleep(15)
    # find lock file and monitor
    logger.info("Monitoring DAG via ID...")
    states = ['unready', 'ready', 'queued', 'failed', 'done']
    colors = ['white', 'magenta', 'blue', 'red', 'green']
    old = dict((k, -1) for k in states)
    logger.debug("Dag Status (%d nodes total):" % (len(dag.get_nodes())))
    logger.debug('-' * 47)
    logger.debug(' {0} |  {1} |  {2} |  {3} |    {4}'.format(
             *[log.color_text(s, c) for s, c in zip(states, colors)]))
    logger.debug('-' * 47)
    for job in condor.iterate_dag_status(dagid):
        for state in states:
            if job[state] != old[state]:
                logger.debug(' | '.join(['%7s' % job[s] for s in states]))
                break
        old = job
    logger.debug('-' * 47)
    if job['exitcode']:
        logger.critical("DAG has exited with status %d" % job['exitcode'])
    else:
        logger.info("DAG has exited with status %d" % job['exitcode'])
    if job['exitcode'] == 0:
        break
    elif i == args.submit_rescue_dag:
        raise RuntimeError("DAG has failed to complete %d times"
                           % args.submit_rescue_dag + 1)
    else:
        sleep(2)
        rescue = condor.find_rescue_dag(dagfile)
        logger.warning("Rescue DAG %s was generated" % rescue)

# write segments
segments.write_segments(span, segfile)
logger.info("Segments written to\n%s" % segfile)

# archive files
stub = '%d-%d' % (start, end)
for f in ['%s.dagman.out' % dagfile, segfile, cachefile, parfile]:
    base, ext = os.path.splitext(os.path.basename(f))
    archive = os.path.join(logdir, '%s-%s%s' % (base, stub, ext))
    shutil.copyfile(f, archive)
    logger.debug("Archived file\n%s --> %s" % (f, archive))

# clean up temporary files
tempfiles.extend(glob(os.path.join(rundir, 'triggers', 'ffconvert.*.ffl')))
for f in tempfiles:
    os.remove(f)
    logger.debug('Deleted file %r' % f)

# and exit
logger.info("---- Processing complete -----------")
